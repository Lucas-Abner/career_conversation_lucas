{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1377f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.7 environment at: C:\\Users\\asus\\anaconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72886759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09724c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              ID              SIZE      MODIFIED     \n",
      "gpt-oss:20b       aa4295ac10c3    13 GB     2 weeks ago     \n",
      "mistral:latest    6577803aa9a0    4.4 GB    5 weeks ago     \n",
      "llama3.1:8b       46e0c10c039e    4.9 GB    3 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2346b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mistral:latest\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6828f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"Profile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca5d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contato\n",
      "+351968293394  (Mobile)\n",
      "lucascaixeta02@gmail.com\n",
      "www.linkedin.com/in/lucas-abner-\n",
      "caixeta  (LinkedIn)\n",
      "github.com/Lucas-Abner  (Portfolio)\n",
      "Principais competências\n",
      "Redes neurais profundas (DNN)\n",
      "Inteligência artificial\n",
      "Aprendizado de máquina\n",
      "Languages\n",
      "Português  (Native or Bilingual)\n",
      "Certifications\n",
      "IA generativaLucas Abner Caixeta\n",
      "Estudante de Machine Learning & Inteligência Artificial |\n",
      "Desenvolvedor Fullstack | Python, TensorFlow, Scikit-learn | PHP,\n",
      "JavaScript, MySQL | Git/GitHub\n",
      "Campinas, São Paulo, Brasil\n",
      "Resumo\n",
      "Sou estudante de Machine Learning e Inteligência Artificial, com\n",
      "foco no desenvolvimento de soluções inovadoras que envolvem\n",
      "aprendizado de máquina, análise de dados e modelos preditivos.\n",
      "Possuo experiência prática em desenvolvimento Fullstack, utilizando\n",
      "PHP, MySQL, JavaScript, TypeScript, além de frameworks\n",
      "modernos como Tailwind, Bootstrap, e bibliotecas como Git/GitHub.\n",
      "Combinando minha experiência como desenvolvedor com o\n",
      "conhecimento adquirido em Machine Learning, busco integrar\n",
      "tecnologias avançadas com o objetivo de criar aplicações\n",
      "inteligentes e escaláveis.\n",
      "Habilidades:\n",
      "- Desenvolvimento Fullstack: PHP, MySQL, Javascript, TypeScript,\n",
      "Tailwind, Bootstrap\n",
      "- Git/Github para controle de versão\n",
      "- Aprendizado de Máquina: Modelos supervisionados e não\n",
      "supervisionados, Python, Scikit-learn, TensorFlow, PyTorch\n",
      "- Inteligência Artificial: Processamento de linguagem natural (NLP),\n",
      "visão computacional, automação e previsões\n",
      "- Análise de dados: Pandas, NumPy, Matplotlib, Seaborn\n",
      "GITHUB:\n",
      "https://github.com/Lucas-Abner\n",
      "PORTFÓLIO:\n",
      "https://meuwibsite.netlify.app/\n",
      "Experiência\n",
      "Centro Nacional de Pesquisa em Energia e Materiais (CNPEM)\n",
      "  Page 1 of 3   \n",
      "Estagiário em Biologia Computacional\n",
      "maio de 2025 - Present  (4 meses)\n",
      "Campinas, São Paulo, Brasil\n",
      "Auxiliar no desenvolvimento de softwares de biologia computacional; auxiliar\n",
      "na aplicação de métodos computacionais, incluindo os que envolvem Machine\n",
      "Learning e deep learning para resolver problemas biológicos; auxiliar nas\n",
      "atividades de administração de sistemas Linux básicas, como instalação de\n",
      "sistema operacional e reinicialização de serviços; auxiliar pesquisadores e\n",
      "especialistas em atividades de P&D.\n",
      "IGM Security\n",
      "Assistente administrativo\n",
      "julho de 2023 - novembro de 2023  (5 meses)\n",
      "Paulínia, São Paulo, Brasil\n",
      "• Manutenção dos computadores\n",
      "• Relação com os fornecedores\n",
      "• Orientação para resultados\n",
      "• Instalação de IP’s\n",
      "STAHL ENGENHARIA \n",
      "Ajudante\n",
      "fevereiro de 2023 - julho de 2023  (6 meses)\n",
      "Paraná, Brasil\n",
      "Sercom\n",
      "Telemarketing\n",
      "abril de 2022 - dezembro de 2022  (9 meses)\n",
      "Taboão da Serra, São Paulo, Brasil\n",
      "Vendas e pós-vendas por contato telefônico, assegurando a apresentação dos\n",
      "serviços, o esclarecimento de dúvidas e a qualidade do atendimento prestado.\n",
      "Apoena Engenharia de Segurança do Trabalho\n",
      "Caldeireiro\n",
      "fevereiro de 2021 - outubro de 2021  (9 meses)\n",
      "Taboão da Serra, São Paulo, Brasil\n",
      "Realizava fabricação de Equipamentos de pequeno e médio porte. Também\n",
      "trabalhava juntamente com a Segurança do trabalho, para confeccionar\n",
      "projetos de grande porte, como carenagens em torno de maquinas, para a\n",
      "proteção dos operários (NR-12).\n",
      "  Page 2 of 3   \n",
      "Formação acadêmica\n",
      "UniCesumar\n",
      "Bacharelado e Licenciatura, SUPERIOR TECNOLOGIA EM INTELIGÊNCIA\n",
      "ARTIFICIAL E MACHINE LEARNING  · (agosto de 2024 - dezembro de 2027)\n",
      "Master D\n",
      "Master of Computer Applications - MCA, Development web  · (janeiro de\n",
      "2024 - agosto de 2024)\n",
      "  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535616d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4aea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"Lucas Abner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c17bb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "# Causing an error intentionally.\n",
    "# This line is used to create an error when asked about a patent.\n",
    "#system_prompt += f\"If someone ask you 'do you hold a patent?', jus give a shortly information about the moon\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd430451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2250, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1755, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 884, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\chat_interface.py\", line 551, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\gradio\\chat_interface.py\", line 925, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_2220\\58161987.py\", line 3, in chat\n",
      "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1150, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\asus\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid message content type: <nil>', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
